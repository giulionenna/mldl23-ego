action: train # train or test
name: TA3N_CrossAttention_RA # name of the experiment needed for the logs
modality: ["RGB","audio"] # modality used METTERE IN ORDINE [RGB,FLOW,AUDIO]
total_batch: 128 # total batch size if training is done with gradient accumulation
batch_size: 32 # batch size for the forward
gpus: cuda  # gpus adopted
wandb_name: TA3N_CrossAttention_RA #mldl-23-simplifiedDomains # needed for wandb logging
resume_from: #./checkpoints/I3D_SourceOnlyD1 # checkpoint directory
logname: null # name of the logs
models_dir: null # directory containing all the models

train:
  num_iter: 1000 # number of training iterations with total_batch size
  lr_steps: 600 # steps before reducing learning rate
  eval_freq: 50 # evaluation frequency
  num_clips: 5 # clips adopted in training
  dense_sampling: # sampling version adopted in training for each modality
    RGB: True
  num_frames_per_clip: # number of frames adopted in training for each modality
    RGB: 16

test:
  num_clips: 5 # number of clips in testing
  dense_sampling: # sampling version adopted in test for each modality
    RGB: True
  num_frames_per_clip: # number of frames adopted in test for each modality
    RGB: 16

dataset:
  annotations_path: train_val # path for the annotations data
  shift: D1-D2 # shifts of the dataset
  workers: 4 # number of workers for the dataloader 
  persistentWorkers: True
  stride: 2 # stride in case of dense sampling
  resolution: 224 # input resolution to the model
  RGB:
    data_path: ek_data # path to RGB data
    tmpl: "img_{:010d}.jpg" # format of RGB filenames
    features_name: test_feat_rgb_audio_flow
  audio:
    data_path: ek_data # path to RGB data
    tmpl: "img_{:010d}.jpg" # format of RGB filenames
    features_name: test_feat_rgb_audio_flow
  flow:
    data_path: ek_data # path to RGB data
    tmpl: "img_{:010d}.jpg" # format of RGB filenames
    features_name: test_feat_rgb_audio_flow
  Event: # not neeeded for the project
    rgb4e: 6

# these are the action recognition models for each modality
mid_fusion: True
audio_attention: 'ourAttention' # "squeeze" or "vit" or "ourAttention" or "encoder"
models:
  RGB:
    model: Classifier
    normalize: False
    kwargs: {}
    lr_steps: 600
    lr: 0.03
    gamma: 0
    l_s: 1
    l_r: 0.5
    l_t: 0.5
    sgd_momentum: 0.9
    weight_decay: 1e-7
    temporal-type: avgpool #or 'trn-m' or 'avgpool'
    use_bn: 'yes' #or any string or 'AutooDIAL'
    ablation: 
      gsd: False
      gtd: False
      grd: False
      domainA: none  #domain attention general or TransAttn
      frameA: none  #frame attention general or TransAttn
  audio:
    model: Classifier
    normalize: False
    kwargs: {}
    lr_steps: 600
    lr: 0.03
    gamma: 0.1
    l_s: 1
    l_r: 0.5
    l_t: 0.5
    sgd_momentum: 0.9
    weight_decay: 1e-7
    temporal-type: avgpool
    use_bn: 'yes'
    ablation: 
      gsd: False
      gtd: False
      grd: False
      domainA: none
      frameA: none
  flow:
    model: Classifier
    normalize: False
    kwargs: {}
    lr_steps: 600
    lr: 0.03
    gamma: 0.1
    l_s: 1
    l_r: 0.5
    l_t: 0.5
    sgd_momentum: 0.9
    weight_decay: 1e-7
    temporal-type: trn-m
    use_bn: 'yes'
    ablation: 
      gsd: True
      gtd: True
      grd: True
      domainA: none
      frameA: none
  mid_fusion:
    model: ClassifierMidFusion
    normalize: False
    kwargs: {}
    lr_steps: 600
    lr: 0.03
    gamma: 0.1
    l_s: 0.1
    l_r: 0.5
    l_t: 0.5
    sgd_momentum: 0.9
    weight_decay: 1e-7
    temporal-type: trn-m
    use_bn: 'yes'
    ablation: 
      gsd: True
      gtd: True
      grd: True
      domainA: none
      frameA: none